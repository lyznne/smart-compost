{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the MNIST dataset\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Normalize the data\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "# Building a simple neural network model\n",
    "model = models.Sequential(\n",
    "    [\n",
    "        layers.Flatten(input_shape=(28, 28)),\n",
    "        layers.Dense(128, activation=\"relu\"),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.Dense(10, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Compile the model with loss function and optimizer\n",
    "model.compile(\n",
    "    optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "# Train the model with training and validation data\n",
    "history = model.fit(x_train, y_train, epochs=10, validation_data=(x_test, y_test))\n",
    "\n",
    "# Plotting Training and Validation Loss\n",
    "plt.plot(history.history[\"loss\"], label=\"Training Loss\")\n",
    "plt.plot(history.history[\"val_loss\"], label=\"Validation Loss\")\n",
    "plt.title(\"Training and Validation Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Define dataset path dynamically\n",
    "root_dir = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "dataset_path: str = os.path.join(root_dir, \"data\", \"smart_compost_dataset101.csv\")\n",
    "\n",
    "\n",
    "class CompostDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        # Load data using pandas\n",
    "        df = pd.read_csv(dataset_path, delimiter=\",\")\n",
    "\n",
    "        # Select numerical columns for features (X) and target (y)\n",
    "        # Assuming the target variable is the first column (e.g., \"Temperature\")\n",
    "        self.y = torch.tensor(df.iloc[:, 0].values, dtype=torch.float32)  # Target\n",
    "        self.x = torch.tensor(df.iloc[:, 1:].values, dtype=torch.float32)  # Features\n",
    "        self.n_samples = df.shape[0]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Return a single sample\n",
    "        return self.x[index], self.y[index]\n",
    "\n",
    "\n",
    "# Instantiate dataset\n",
    "dataset = CompostDataset()\n",
    "first_data = dataset[0]\n",
    "features, labels = first_data\n",
    "print(\"Features:\", features)\n",
    "print(\"Labels:\", labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class CompostDataset(Dataset):\n",
    "    def __init__(self, csv_path):\n",
    "        # Load data\n",
    "        df = pd.read_csv(csv_path, delimiter=\",\", skip_blank_lines=True)\n",
    "\n",
    "        # Clean column names\n",
    "        df.columns = df.columns.str.strip()\n",
    "\n",
    "        # Remove any metadata or header rows (keeping only the actual data)\n",
    "        df = df[df[\"Variable\"].notna()]\n",
    "\n",
    "        # Create a mapping of variable names to numeric indices\n",
    "        self.variable_mapping = {\n",
    "            var: idx for idx, var in enumerate(df[\"Variable\"].unique())\n",
    "        }\n",
    "\n",
    "        # Convert optimal ranges to numeric values (taking the midpoint)\n",
    "        def extract_range_midpoint(range_str):\n",
    "            try:\n",
    "                # Handle special cases like '>7'\n",
    "                if \">\" in range_str:\n",
    "                    return float(range_str.replace(\">\", \"\"))\n",
    "\n",
    "                # Handle normal ranges like '45-65'\n",
    "                low, high = map(float, range_str.split(\"-\"))\n",
    "                return (low + high) / 2\n",
    "            except:\n",
    "                return None\n",
    "\n",
    "        df[\"OptimalRange\"] = (\n",
    "            df[\"OptimalRange\"].str.strip().apply(extract_range_midpoint)\n",
    "        )\n",
    "\n",
    "        # Select numeric columns for the feature set\n",
    "        numeric_columns = [\"OptimalRange\"]  # Add other numeric columns as needed\n",
    "\n",
    "        # Convert to numeric, handling errors\n",
    "        for col in numeric_columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "\n",
    "        # Drop rows with NaN values\n",
    "        df = df.dropna(subset=numeric_columns)\n",
    "\n",
    "        if df.empty:\n",
    "            raise ValueError(\n",
    "                \"Dataset is empty after preprocessing. Please check your data format.\"\n",
    "            )\n",
    "\n",
    "        # Store processed data\n",
    "        self.variable_names = df[\"Variable\"].values\n",
    "        self.features = torch.tensor(df[numeric_columns].values, dtype=torch.float32)\n",
    "        self.n_samples = len(df)\n",
    "\n",
    "        # Store additional metadata that might be useful\n",
    "        self.metadata = df[\n",
    "            [\"Type\", \"Unit\", \"Dependencies\", \"IntroductionStage\", \"Frequency\", \"Notes\"]\n",
    "        ]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        variable_name = self.variable_names[index]\n",
    "        variable_idx = torch.tensor(\n",
    "            self.variable_mapping[variable_name], dtype=torch.long\n",
    "        )\n",
    "        return self.features[index], variable_idx\n",
    "\n",
    "    def get_metadata(self, index):\n",
    "        \"\"\"Return metadata for a given index\"\"\"\n",
    "        return self.metadata.iloc[index]\n",
    "\n",
    "\n",
    "\n",
    "root_dir = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "dataset_path = os.path.join(root_dir, \"data\", \"smart_compost_dataset101.csv\")\n",
    "\n",
    "try:\n",
    "    dataset = CompostDataset(dataset_path)\n",
    "    # print(f\"Dataset size: {len(dataset)}\")\n",
    "\n",
    "    # Get first item\n",
    "    # features, variable_idx = dataset[0]\n",
    "    # print(f\"Features for first item: {features}\")\n",
    "    # print(f\"Variable index for first item: {variable_idx}\")\n",
    "\n",
    "    # Get metadata for first item\n",
    "    metadata = dataset.get_metadata(0)\n",
    "    # print(f\"Metadata for first item: {metadata}\")\n",
    "\n",
    "\n",
    "    # dataloader\n",
    "    dataloader =  DataLoader(dataset=dataset, batch_size=4, shuffle=True, num_workers=2)\n",
    "\n",
    "\n",
    "    datatiter  = iter(dataloader)\n",
    "    data  =  datatiter.__next__()\n",
    "    features, labels = data\n",
    "\n",
    "    print(f'features: {features}')\n",
    "    print(f'Labels : {labels}')\n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class CompostDataset(Dataset):\n",
    "    def __init__(self, csv_path):\n",
    "        # Load data\n",
    "        df = pd.read_csv(csv_path, delimiter=\",\", skip_blank_lines=True)\n",
    "\n",
    "        # Clean column names\n",
    "        df.columns = df.columns.str.strip()\n",
    "\n",
    "        # Remove any metadata or header rows (keeping only the actual data)\n",
    "        df = df[df[\"Variable\"].notna()]\n",
    "\n",
    "        # Create a mapping of variable names to numeric indices\n",
    "        self.variable_mapping = {\n",
    "            var: idx for idx, var in enumerate(df[\"Variable\"].unique())\n",
    "        }\n",
    "\n",
    "        # Convert optimal ranges to numeric values (taking the midpoint)\n",
    "        def extract_range_midpoint(range_str):\n",
    "            try:\n",
    "                if \">\" in range_str:\n",
    "                    return float(range_str.replace(\">\", \"\"))\n",
    "                low, high = map(float, range_str.split(\"-\"))\n",
    "                return (low + high) / 2\n",
    "            except:\n",
    "                return None\n",
    "\n",
    "        df[\"OptimalRange\"] = (\n",
    "            df[\"OptimalRange\"].str.strip().apply(extract_range_midpoint)\n",
    "        )\n",
    "\n",
    "        # Encode categorical columns as numeric\n",
    "        df[\"Frequency\"] = df[\"Frequency\"].astype(\"category\").cat.codes\n",
    "        df[\"IntroductionStage\"] = df[\"IntroductionStage\"].astype(\"category\").cat.codes\n",
    "\n",
    "        # Add these columns to the feature set\n",
    "        numeric_columns = [\"OptimalRange\", \"Frequency\", \"IntroductionStage\"]\n",
    "\n",
    "        # Normalize features\n",
    "        df[numeric_columns] = df[numeric_columns].apply(\n",
    "            lambda x: (x - x.mean()) / x.std()\n",
    "        )\n",
    "\n",
    "        # Convert to numeric and handle errors\n",
    "        for col in numeric_columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "\n",
    "        # Drop rows with NaN values\n",
    "        df = df.dropna(subset=numeric_columns)\n",
    "\n",
    "        if df.empty:\n",
    "            raise ValueError(\n",
    "                \"Dataset is empty after preprocessing. Please check your data format.\"\n",
    "            )\n",
    "\n",
    "        # Store processed data\n",
    "        self.variable_names = df[\"Variable\"].values\n",
    "        self.features = torch.tensor(df[numeric_columns].values, dtype=torch.float32)\n",
    "        self.labels = torch.tensor(\n",
    "            [self.variable_mapping[var] for var in df[\"Variable\"].values],\n",
    "            dtype=torch.long,\n",
    "        )\n",
    "        self.n_samples = len(df)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.features[index], self.labels[index]\n",
    "\n",
    "\n",
    "root_dir = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "dataset_path = os.path.join(root_dir, \"data\", \"smart_compost_dataset101.csv\")\n",
    "\n",
    "try:\n",
    "    dataset = CompostDataset(dataset_path)\n",
    "\n",
    "    # DataLoader\n",
    "    dataloader = DataLoader(dataset=dataset, batch_size=4, shuffle=True, num_workers=2)\n",
    "\n",
    "    data_iter = iter(dataloader)\n",
    "    features, labels = next(data_iter)\n",
    "\n",
    "    print(f\"Features: {features}\")\n",
    "    print(f\"Labels: {labels}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class CompostDataset(Dataset):\n",
    "    def __init__(self, csv_path):\n",
    "        # Load data\n",
    "        df = pd.read_csv(csv_path, delimiter=\",\", skip_blank_lines=True)\n",
    "\n",
    "        # Clean column names\n",
    "        df.columns = df.columns.str.strip()\n",
    "\n",
    "        # Remove any metadata or header rows\n",
    "        df = df[df[\"Variable\"].notna()]\n",
    "\n",
    "        # Create feature mappings\n",
    "        self.variable_mapping = {\n",
    "            var: idx for idx, var in enumerate(df[\"Variable\"].unique())\n",
    "        }\n",
    "\n",
    "        # Convert all ranges to numeric values\n",
    "        def extract_range_values(range_str):\n",
    "            try:\n",
    "                if isinstance(range_str, str):\n",
    "                    range_str = range_str.strip()\n",
    "                    if \">\" in range_str:\n",
    "                        return (\n",
    "                            float(range_str.replace(\">\", \"\")),\n",
    "                            float(range_str.replace(\">\", \"\")) * 1.2,\n",
    "                        )\n",
    "                    elif \":\" in range_str:  # Handle ratios like \"25-30:1\"\n",
    "                        range_part = range_str.split(\":\")[0]\n",
    "                        low, high = map(float, range_part.split(\"-\"))\n",
    "                        return low, high\n",
    "                    else:\n",
    "                        low, high = map(float, range_str.split(\"-\"))\n",
    "                        return low, high\n",
    "                return None, None\n",
    "            except:\n",
    "                return None, None\n",
    "\n",
    "        # Extract ranges and create feature bounds\n",
    "        ranges = df[\"OptimalRange\"].apply(extract_range_values)\n",
    "        df[\"lower_bound\"] = ranges.apply(lambda x: x[0])\n",
    "        df[\"upper_bound\"] = ranges.apply(lambda x: x[1])\n",
    "\n",
    "        # Create feature matrix\n",
    "        feature_cols = [\"lower_bound\", \"upper_bound\"]\n",
    "        self.features = torch.tensor(df[feature_cols].values, dtype=torch.float32)\n",
    "\n",
    "        # Create normalized features\n",
    "        self.features = self.normalize_features(self.features)\n",
    "\n",
    "        # Store variable information\n",
    "        self.variables = df[\"Variable\"].values\n",
    "        self.units = df[\"Unit\"].values\n",
    "        self.types = df[\"Type\"].values\n",
    "        self.dependencies = df[\"Dependencies\"].str.split(\"|\").values\n",
    "        self.n_samples = len(df)\n",
    "\n",
    "        # Store metadata\n",
    "        self.metadata = {\n",
    "            \"variable_names\": self.variables,\n",
    "            \"units\": self.units,\n",
    "            \"types\": self.types,\n",
    "            \"dependencies\": self.dependencies,\n",
    "        }\n",
    "\n",
    "    def normalize_features(self, features):\n",
    "        \"\"\"Normalize features to [0,1] range\"\"\"\n",
    "        min_vals = features.min(dim=0)[0]\n",
    "        max_vals = features.max(dim=0)[0]\n",
    "        return (features - min_vals) / (max_vals - min_vals)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Return features and variable index for the given index\"\"\"\n",
    "        features = self.features[index]\n",
    "        variable_idx = torch.tensor(\n",
    "            self.variable_mapping[self.variables[index]], dtype=torch.long\n",
    "        )\n",
    "\n",
    "        # Create dependency mask\n",
    "        deps = self.dependencies[index]\n",
    "        if isinstance(deps, (list, np.ndarray)):\n",
    "            dep_mask = torch.zeros(len(self.variable_mapping))\n",
    "            for dep in deps:\n",
    "                if dep in self.variable_mapping and dep != \"All_Above_Variables\":\n",
    "                    dep_mask[self.variable_mapping[dep]] = 1\n",
    "            return features, variable_idx, dep_mask\n",
    "        return features, variable_idx, torch.zeros(len(self.variable_mapping))\n",
    "\n",
    "    def get_variable_name(self, index):\n",
    "        \"\"\"Get variable name for given index\"\"\"\n",
    "        return self.variables[index]\n",
    "\n",
    "    def get_unit(self, index):\n",
    "        \"\"\"Get unit for given index\"\"\"\n",
    "        return self.units[index]\n",
    "\n",
    "    def get_dependencies(self, index):\n",
    "        \"\"\"Get dependencies for given index\"\"\"\n",
    "        return self.dependencies[index]\n",
    "\n",
    "    def get_optimal_range(self, index):\n",
    "        \"\"\"Get optimal range for given index\"\"\"\n",
    "        return self.features[index].numpy()\n",
    "\n",
    "\n",
    "# Example usage\n",
    "def create_dataloader(dataset_path, batch_size=4, shuffle=True):\n",
    "    try:\n",
    "        dataset = CompostDataset(dataset_path)\n",
    "        dataloader = DataLoader(\n",
    "            dataset=dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=shuffle,\n",
    "            num_workers=2,\n",
    "            collate_fn=lambda x: tuple(zip(*x)),  # Handle variable length dependencies\n",
    "        )\n",
    "        return dataset, dataloader\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating dataloader: {e}\")\n",
    "        return None, None\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    root_dir = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "    dataset_path = os.path.join(root_dir, \"data\", \"smart_compost_dataset101.csv\")\n",
    "\n",
    "    dataset, dataloader = create_dataloader(dataset_path)\n",
    "    if dataloader:\n",
    "        # Get a batch of data\n",
    "        data_iter = iter(dataloader)\n",
    "        features, variable_indices, dependency_masks = next(data_iter)\n",
    "\n",
    "        print(\"Batch features shape:\", torch.stack(features).shape)\n",
    "        print(\"Batch variable indices:\", variable_indices)\n",
    "        print(\"First item dependencies:\", dataset.get_dependencies(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset and dataloader\n",
    "dataset, dataloader = create_compost_dataloader(\n",
    "    dataset_path, batch_size=32, sequence_length=30\n",
    ")\n",
    "\n",
    "# Training loop example\n",
    "for batch_features, batch_targets in dataloader:\n",
    "    # batch_features: shape [batch_size, sequence_length, n_features]\n",
    "    # batch_targets: shape [batch_size, n_targets]\n",
    "\n",
    "    # Features include:\n",
    "    # - Measured values for all variables\n",
    "    # - Waste type encoding\n",
    "    # - Weather data\n",
    "\n",
    "    # Targets are next-day predictions for:\n",
    "    # - Temperature\n",
    "    # - Moisture content\n",
    "\n",
    "    # Your model training code here\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "\n",
    "class WasteType:\n",
    "    \"\"\"Classification of different waste types and their properties\"\"\"\n",
    "\n",
    "    TYPES = {\n",
    "        \"GREEN\": {\n",
    "            \"examples\": [\"grass\", \"vegetable_scraps\", \"coffee_grounds\", \"fruit_waste\"],\n",
    "            \"nitrogen_content\": \"high\",\n",
    "            \"moisture_content\": \"high\",\n",
    "            \"decomposition_rate\": \"fast\",\n",
    "        },\n",
    "        \"BROWN\": {\n",
    "            \"examples\": [\"leaves\", \"straw\", \"paper\", \"cardboard\", \"wood_chips\"],\n",
    "            \"nitrogen_content\": \"low\",\n",
    "            \"moisture_content\": \"low\",\n",
    "            \"decomposition_rate\": \"slow\",\n",
    "        },\n",
    "        \"MIXED\": {\n",
    "            \"examples\": [\"food_scraps\", \"garden_waste\"],\n",
    "            \"nitrogen_content\": \"medium\",\n",
    "            \"moisture_content\": \"medium\",\n",
    "            \"decomposition_rate\": \"medium\",\n",
    "        },\n",
    "    }\n",
    "\n",
    "    @staticmethod\n",
    "    def get_type_encoding(waste_type: str) -> torch.Tensor:\n",
    "        \"\"\"Convert waste type to one-hot encoding\"\"\"\n",
    "        encoding = torch.zeros(len(WasteType.TYPES))\n",
    "        if waste_type in WasteType.TYPES:\n",
    "            encoding[list(WasteType.TYPES.keys()).index(waste_type)] = 1\n",
    "        return encoding\n",
    "\n",
    "\n",
    "class SeasonalEffects:\n",
    "    \"\"\"Handle seasonal and weather effects on composting\"\"\"\n",
    "\n",
    "    SEASONS = {\n",
    "        \"SPRING\": {\"temp_modifier\": 0.2, \"moisture_modifier\": 0.3},\n",
    "        \"SUMMER\": {\"temp_modifier\": 0.4, \"moisture_modifier\": -0.2},\n",
    "        \"FALL\": {\"temp_modifier\": 0.0, \"moisture_modifier\": 0.1},\n",
    "        \"WINTER\": {\"temp_modifier\": -0.3, \"moisture_modifier\": 0.2},\n",
    "    }\n",
    "\n",
    "    @staticmethod\n",
    "    def get_season(date: datetime) -> str:\n",
    "        \"\"\"Determine season from date\"\"\"\n",
    "        month = date.month\n",
    "        if month in [3, 4, 5]:\n",
    "            return \"SPRING\"\n",
    "        elif month in [6, 7, 8]:\n",
    "            return \"SUMMER\"\n",
    "        elif month in [9, 10, 11]:\n",
    "            return \"FALL\"\n",
    "        else:\n",
    "            return \"WINTER\"\n",
    "\n",
    "\n",
    "class CompostTimeSeriesDataset(Dataset):\n",
    "    def __init__(self, csv_path: str, sequence_length: int = 30):\n",
    "        super().__init__()\n",
    "        self.sequence_length = sequence_length\n",
    "        self.base_data = pd.read_csv(csv_path, delimiter=\",\", skip_blank_lines=True)\n",
    "        self.base_data.columns = self.base_data.columns.str.strip()\n",
    "\n",
    "        # Initialize time series data structures\n",
    "        self.time_series_data = self._generate_time_series_data()\n",
    "        self.waste_types = self._initialize_waste_types()\n",
    "        self.weather_data = self._initialize_weather_data()\n",
    "\n",
    "        # Process optimal ranges\n",
    "        self._process_optimal_ranges()\n",
    "\n",
    "        # Create feature matrices\n",
    "        self._create_feature_matrices()\n",
    "\n",
    "    def _generate_time_series_data(self) -> Dict[str, List[float]]:\n",
    "        \"\"\"Generate synthetic time series data for each variable\"\"\"\n",
    "        time_series = {}\n",
    "        start_date = datetime.now() - timedelta(days=365)\n",
    "        dates = [start_date + timedelta(days=i) for i in range(365)]\n",
    "\n",
    "        for _, row in self.base_data.iterrows():\n",
    "            variable = row[\"Variable\"]\n",
    "            optimal_range = self._parse_range(row[\"OptimalRange\"])\n",
    "            if optimal_range:\n",
    "                mid_point = (optimal_range[0] + optimal_range[1]) / 2\n",
    "                # Generate daily values with realistic variations\n",
    "                values = [\n",
    "                    self._generate_realistic_value(mid_point, optimal_range, dates[i])\n",
    "                    for i in range(365)\n",
    "                ]\n",
    "                time_series[variable] = values\n",
    "\n",
    "        return time_series\n",
    "\n",
    "    def _generate_realistic_value(\n",
    "        self, mid_point: float, optimal_range: Tuple[float, float], date: datetime\n",
    "    ) -> float:\n",
    "        \"\"\"Generate realistic value considering seasonal effects and daily variations\"\"\"\n",
    "        season = SeasonalEffects.get_season(date)\n",
    "        season_effect = SeasonalEffects.SEASONS[season][\"temp_modifier\"]\n",
    "\n",
    "        # Add daily variation (random walk with bounds)\n",
    "        daily_variation = np.random.normal(0, 0.05)\n",
    "        value = mid_point * (1 + season_effect + daily_variation)\n",
    "\n",
    "        # Ensure value stays within optimal range\n",
    "        return max(optimal_range[0], min(optimal_range[1], value))\n",
    "\n",
    "    def _initialize_waste_types(self) -> List[str]:\n",
    "        \"\"\"Initialize waste type sequence\"\"\"\n",
    "        return np.random.choice(list(WasteType.TYPES.keys()), size=365).tolist()\n",
    "\n",
    "    def _initialize_weather_data(self) -> Dict[str, List[float]]:\n",
    "        \"\"\"Generate synthetic weather data\"\"\"\n",
    "        weather_data = {\"temperature\": [], \"humidity\": [], \"rainfall\": []}\n",
    "\n",
    "        start_date = datetime.now() - timedelta(days=365)\n",
    "        for i in range(365):\n",
    "            date = start_date + timedelta(days=i)\n",
    "            season = SeasonalEffects.get_season(date)\n",
    "            season_effects = SeasonalEffects.SEASONS[season]\n",
    "\n",
    "            # Generate weather data with seasonal effects\n",
    "            weather_data[\"temperature\"].append(\n",
    "                20 + season_effects[\"temp_modifier\"] * 30 + np.random.normal(0, 2)\n",
    "            )\n",
    "            weather_data[\"humidity\"].append(\n",
    "                60 + season_effects[\"moisture_modifier\"] * 30 + np.random.normal(0, 5)\n",
    "            )\n",
    "            weather_data[\"rainfall\"].append(\n",
    "                max(\n",
    "                    0,\n",
    "                    np.random.exponential(scale=2)\n",
    "                    * (1 + season_effects[\"moisture_modifier\"]),\n",
    "                )\n",
    "            )\n",
    "\n",
    "        return weather_data\n",
    "\n",
    "    def _parse_range(self, range_str: str) -> Tuple[float, float]:\n",
    "        \"\"\"Parse range string to tuple of floats\"\"\"\n",
    "        try:\n",
    "            if isinstance(range_str, str):\n",
    "                range_str = range_str.strip()\n",
    "                if \">\" in range_str:\n",
    "                    val = float(range_str.replace(\">\", \"\"))\n",
    "                    return (val, val * 1.2)\n",
    "                elif \":\" in range_str:\n",
    "                    range_part = range_str.split(\":\")[0]\n",
    "                    low, high = map(float, range_part.split(\"-\"))\n",
    "                    return (low, high)\n",
    "                else:\n",
    "                    return tuple(map(float, range_str.split(\"-\")))\n",
    "            return None\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "    def _process_optimal_ranges(self):\n",
    "        \"\"\"Process optimal ranges for all variables\"\"\"\n",
    "        self.optimal_ranges = {}\n",
    "        for _, row in self.base_data.iterrows():\n",
    "            range_vals = self._parse_range(row[\"OptimalRange\"])\n",
    "            if range_vals:\n",
    "                self.optimal_ranges[row[\"Variable\"]] = range_vals\n",
    "\n",
    "    def _create_feature_matrices(self):\n",
    "        \"\"\"Create feature matrices for the dataset\"\"\"\n",
    "        self.features = []\n",
    "        self.targets = []\n",
    "\n",
    "        for i in range(\n",
    "            len(self.time_series_data[\"Temperature\"]) - self.sequence_length\n",
    "        ):\n",
    "            # Create sequence of features\n",
    "            sequence = []\n",
    "            for day in range(self.sequence_length):\n",
    "                idx = i + day\n",
    "\n",
    "                # Measurements\n",
    "                measurements = [\n",
    "                    self.time_series_data[var][idx]\n",
    "                    for var in self.time_series_data.keys()\n",
    "                ]\n",
    "\n",
    "                # Waste type encoding\n",
    "                waste_encoding = WasteType.get_type_encoding(self.waste_types[idx])\n",
    "\n",
    "                # Weather data\n",
    "                weather = [self.weather_data[w][idx] for w in self.weather_data.keys()]\n",
    "\n",
    "                # Combine all features\n",
    "                day_features = measurements + waste_encoding.tolist() + weather\n",
    "                sequence.append(day_features)\n",
    "\n",
    "            self.features.append(torch.tensor(sequence, dtype=torch.float32))\n",
    "\n",
    "            # Target is the next day's temperature and moisture content\n",
    "            target_idx = i + self.sequence_length\n",
    "            target = [\n",
    "                self.time_series_data[\"Temperature\"][target_idx],\n",
    "                self.time_series_data[\"Moisture_Content\"][target_idx],\n",
    "            ]\n",
    "            self.targets.append(torch.tensor(target, dtype=torch.float32))\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        return self.features[idx], self.targets[idx]\n",
    "\n",
    "    def get_optimal_ranges(self) -> Dict[str, Tuple[float, float]]:\n",
    "        \"\"\"Return optimal ranges for all variables\"\"\"\n",
    "        return self.optimal_ranges\n",
    "\n",
    "    def get_variable_metadata(self) -> pd.DataFrame:\n",
    "        \"\"\"Return metadata for all variables\"\"\"\n",
    "        return self.base_data[\n",
    "            [\n",
    "                \"Variable\",\n",
    "                \"Type\",\n",
    "                \"Unit\",\n",
    "                \"Dependencies\",\n",
    "                \"IntroductionStage\",\n",
    "                \"Frequency\",\n",
    "                \"Notes\",\n",
    "            ]\n",
    "        ]\n",
    "\n",
    "\n",
    "# Example usage\n",
    "def create_compost_dataloader(\n",
    "    dataset_path: str, batch_size: int = 32, sequence_length: int = 30\n",
    "):\n",
    "    try:\n",
    "        dataset = CompostTimeSeriesDataset(\n",
    "            dataset_path, sequence_length=sequence_length\n",
    "        )\n",
    "        dataloader = DataLoader(\n",
    "            dataset=dataset, batch_size=batch_size, shuffle=True, num_workers=2\n",
    "        )\n",
    "        return dataset, dataloader\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating dataloader: {e}\")\n",
    "        return None, None\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    root_dir = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "    dataset_path = os.path.join(root_dir, \"data\", \"smart_compost_dataset101.csv\")\n",
    "\n",
    "    dataset, dataloader = create_compost_dataloader(dataset_path)\n",
    "    if dataloader:\n",
    "        print('looping ...')\n",
    "        # Get a batch of data\n",
    "        for batch_features, batch_targets in dataloader:\n",
    "            print(\"Batch features shape:\", batch_features.shape)\n",
    "            print(\"Batch targets shape:\", batch_targets.shape)\n",
    "            print(\n",
    "                \"\\nFeature sequence (first day of first sample):\", batch_features[0][0]\n",
    "            )\n",
    "            print(\"Target values:\", batch_targets[0])\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load latest dataset\n",
    "df = pd.read_csv(\"smart_compost_dataset101.csv\")  # Change file as needed\n",
    "\n",
    "# Set seaborn style\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "\n",
    "# Function to plot variable trends over time\n",
    "def plot_variable(df, variable):\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.plot(df[\"Date\"], df[variable], label=variable, color=\"blue\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(variable)\n",
    "    plt.title(f\"{variable} Over Time\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Example: Plot temperature and moisture trends\n",
    "plot_variable(df, \"Temperature\")\n",
    "plot_variable(df, \"Moisture_Content\")\n",
    "\n",
    "# Correlation heatmap\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(df.drop(columns=[\"Date\", \"Waste_Type\"]).corr(), annot=True, cmap=\"coolwarm\")\n",
    "plt.title(\"Variable Correlation Heatmap\")\n",
    "plt.show()\n",
    "\n",
    "# Scatterplot: Temperature vs Moisture Content\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.scatterplot(x=df[\"Temperature\"], y=df[\"Moisture_Content\"], hue=df[\"Waste_Type\"])\n",
    "plt.title(\"Temperature vs Moisture Content\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnn\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01moptim\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01moptim\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "from typing import Tuple, Dict\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class CompostLSTM(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size: int,\n",
    "        hidden_size: int = 64,\n",
    "        num_layers: int = 2,\n",
    "        dropout: float = 0.2,\n",
    "    ):\n",
    "        super(CompostLSTM, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # LSTM layer\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "        )\n",
    "\n",
    "        # Attention layer\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size), nn.Tanh(), nn.Linear(hidden_size, 1)\n",
    "        )\n",
    "\n",
    "        # Output layers for temperature and moisture prediction\n",
    "        self.regression_head = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_size // 2, 2),  # 2 outputs: temperature and moisture\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # LSTM forward pass\n",
    "        lstm_out, _ = self.lstm(x)  # Shape: [batch, seq_len, hidden_size]\n",
    "\n",
    "        # Apply attention\n",
    "        attention_weights = self.attention(lstm_out)  # Shape: [batch, seq_len, 1]\n",
    "        attention_weights = torch.softmax(attention_weights, dim=1)\n",
    "\n",
    "        # Weighted sum of LSTM outputs\n",
    "        context = torch.sum(\n",
    "            attention_weights * lstm_out, dim=1\n",
    "        )  # Shape: [batch, hidden_size]\n",
    "\n",
    "        # Generate predictions\n",
    "        predictions = self.regression_head(context)  # Shape: [batch, 2]\n",
    "\n",
    "        return predictions\n",
    "\n",
    "\n",
    "class CompostModelTrainer:\n",
    "    def __init__(self, model: nn.Module, learning_rate: float = 0.001):\n",
    "        self.model = model\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model.to(self.device)\n",
    "\n",
    "        # Training history\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "\n",
    "    def train_epoch(self, train_loader: DataLoader) -> float:\n",
    "        self.model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        for batch_features, batch_targets in train_loader:\n",
    "            batch_features = batch_features.to(self.device)\n",
    "            batch_targets = batch_targets.to(self.device)\n",
    "\n",
    "            # Forward pass\n",
    "            self.optimizer.zero_grad()\n",
    "            predictions = self.model(batch_features)\n",
    "            loss = self.criterion(predictions, batch_targets)\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        return total_loss / len(train_loader)\n",
    "\n",
    "    def validate(self, val_loader: DataLoader) -> float:\n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch_features, batch_targets in val_loader:\n",
    "                batch_features = batch_features.to(self.device)\n",
    "                batch_targets = batch_targets.to(self.device)\n",
    "\n",
    "                predictions = self.model(batch_features)\n",
    "                loss = self.criterion(predictions, batch_targets)\n",
    "                total_loss += loss.item()\n",
    "\n",
    "        return total_loss / len(val_loader)\n",
    "\n",
    "    def train(\n",
    "        self,\n",
    "        train_loader: DataLoader,\n",
    "        val_loader: DataLoader,\n",
    "        epochs: int = 100,\n",
    "        early_stopping_patience: int = 10,\n",
    "    ) -> Dict:\n",
    "\n",
    "        best_val_loss = float(\"inf\")\n",
    "        patience_counter = 0\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            # Train and validate\n",
    "            train_loss = self.train_epoch(train_loader)\n",
    "            val_loss = self.validate(val_loader)\n",
    "\n",
    "            # Store losses\n",
    "            self.train_losses.append(train_loss)\n",
    "            self.val_losses.append(val_loss)\n",
    "\n",
    "            # Print progress\n",
    "            print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "            print(f\"Train Loss: {train_loss:.4f}\")\n",
    "            print(f\"Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "            # Early stopping check\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                patience_counter = 0\n",
    "                # Save best model\n",
    "                torch.save(self.model.state_dict(), \"best_compost_model.pth\")\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "\n",
    "            if patience_counter >= early_stopping_patience:\n",
    "                print(\"Early stopping triggered!\")\n",
    "                break\n",
    "\n",
    "        return {\"train_losses\": self.train_losses, \"val_losses\": self.val_losses}\n",
    "\n",
    "    def plot_training_history(self):\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(self.train_losses, label=\"Training Loss\")\n",
    "        plt.plot(self.val_losses, label=\"Validation Loss\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.title(\"Training History\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's what this model implementation provides:\n",
    "\n",
    "Model Architecture:\n",
    "\n",
    "LSTM-based network for time series processing\n",
    "Attention mechanism to focus on important time steps\n",
    "Dropout for regularization\n",
    "Separate regression heads for temperature and moisture prediction\n",
    "\n",
    "\n",
    "Training Infrastructure:\n",
    "\n",
    "Comprehensive training loop with validation\n",
    "Early stopping to prevent overfitting\n",
    "Learning rate optimization with Adam\n",
    "Loss tracking and visualization\n",
    "\n",
    "\n",
    "\n",
    "To use this model:\n",
    "\n",
    "First, prepare your data:\n",
    "```python \n",
    "dataset_path = \"path_to_your_dataset.csv\"\n",
    "dataset, full_dataloader = create_compost_dataloader(dataset_path)\n",
    "\n",
    "# Split into train/validation sets\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step would be to create a neural network model to train on this composting data. Let me create an appropriate model architecture for this task.\n",
    "\n",
    "\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "from typing import Tuple, Dict\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class CompostLSTM(nn.Module):\n",
    "    def __init__(self, input_size: int, hidden_size: int = 64, num_layers: int = 2, dropout: float = 0.2):\n",
    "        super(CompostLSTM, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # LSTM layer\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0\n",
    "        )\n",
    "\n",
    "        # Attention layer\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size, 1)\n",
    "        )\n",
    "\n",
    "        # Output layers for temperature and moisture prediction\n",
    "        self.regression_head = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_size // 2, 2)  # 2 outputs: temperature and moisture\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # LSTM forward pass\n",
    "        lstm_out, _ = self.lstm(x)  # Shape: [batch, seq_len, hidden_size]\n",
    "\n",
    "        # Apply attention\n",
    "        attention_weights = self.attention(lstm_out)  # Shape: [batch, seq_len, 1]\n",
    "        attention_weights = torch.softmax(attention_weights, dim=1)\n",
    "\n",
    "        # Weighted sum of LSTM outputs\n",
    "        context = torch.sum(attention_weights * lstm_out, dim=1)  # Shape: [batch, hidden_size]\n",
    "\n",
    "        # Generate predictions\n",
    "        predictions = self.regression_head(context)  # Shape: [batch, 2]\n",
    "\n",
    "        return predictions\n",
    "\n",
    "class CompostModelTrainer:\n",
    "    def __init__(self, model: nn.Module, learning_rate: float = 0.001):\n",
    "        self.model = model\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model.to(self.device)\n",
    "\n",
    "        # Training history\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "\n",
    "    def train_epoch(self, train_loader: DataLoader) -> float:\n",
    "        self.model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        for batch_features, batch_targets in train_loader:\n",
    "            batch_features = batch_features.to(self.device)\n",
    "            batch_targets = batch_targets.to(self.device)\n",
    "\n",
    "            # Forward pass\n",
    "            self.optimizer.zero_grad()\n",
    "            predictions = self.model(batch_features)\n",
    "            loss = self.criterion(predictions, batch_targets)\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        return total_loss / len(train_loader)\n",
    "\n",
    "    def validate(self, val_loader: DataLoader) -> float:\n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch_features, batch_targets in val_loader:\n",
    "                batch_features = batch_features.to(self.device)\n",
    "                batch_targets = batch_targets.to(self.device)\n",
    "\n",
    "                predictions = self.model(batch_features)\n",
    "                loss = self.criterion(predictions, batch_targets)\n",
    "                total_loss += loss.item()\n",
    "\n",
    "        return total_loss / len(val_loader)\n",
    "\n",
    "    def train(self,\n",
    "              train_loader: DataLoader,\n",
    "              val_loader: DataLoader,\n",
    "              epochs: int = 100,\n",
    "              early_stopping_patience: int = 10) -> Dict:\n",
    "\n",
    "        best_val_loss = float('inf')\n",
    "        patience_counter = 0\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            # Train and validate\n",
    "            train_loss = self.train_epoch(train_loader)\n",
    "            val_loss = self.validate(val_loader)\n",
    "\n",
    "            # Store losses\n",
    "            self.train_losses.append(train_loss)\n",
    "            self.val_losses.append(val_loss)\n",
    "\n",
    "            # Print progress\n",
    "            print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "            print(f\"Train Loss: {train_loss:.4f}\")\n",
    "            print(f\"Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "            # Early stopping check\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                patience_counter = 0\n",
    "                # Save best model\n",
    "                torch.save(self.model.state_dict(), 'best_compost_model.pth')\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "\n",
    "            if patience_counter >= early_stopping_patience:\n",
    "                print(\"Early stopping triggered!\")\n",
    "                break\n",
    "\n",
    "        return {\n",
    "            'train_losses': self.train_losses,\n",
    "            'val_losses': self.val_losses\n",
    "        }\n",
    "\n",
    "    def plot_training_history(self):\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(self.train_losses, label='Training Loss')\n",
    "        plt.plot(self.val_losses, label='Validation Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Training History')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "# Example usage\n",
    "def train_compost_model(train_loader: DataLoader, val_loader: DataLoader) -> CompostLSTM:\n",
    "    # Calculate input size from data\n",
    "    sample_features = next(iter(train_loader))[0]\n",
    "    input_size = sample_features.shape[-1]  # number of features\n",
    "\n",
    "    # Initialize model and trainer\n",
    "    model = CompostLSTM(input_size=input_size)\n",
    "    trainer = CompostModelTrainer(model)\n",
    "\n",
    "    # Train the model\n",
    "    history = trainer.train(\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        epochs=100,\n",
    "        early_stopping_patience=10\n",
    "    )\n",
    "\n",
    "    # Plot training history\n",
    "    trainer.plot_training_history()\n",
    "\n",
    "    return model\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Assuming you have already created your dataset and dataloaders\n",
    "    from compost_dataset import create_compost_dataloader\n",
    "\n",
    "    # Create dataset and split into train/val\n",
    "    dataset_path = \"path_to_your_dataset.csv\"\n",
    "    dataset, full_dataloader = create_compost_dataloader(dataset_path)\n",
    "\n",
    "    # Split data into train and validation sets\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    val_size = len(dataset) - train_size\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    # Train the model\n",
    "    model = train_compost_model(train_loader, val_loader)\n",
    "\n",
    "```\n",
    "\n",
    "Here's what this model implementation provides:\n",
    "\n",
    "1. Model Architecture:\n",
    "   - LSTM-based network for time series processing\n",
    "   - Attention mechanism to focus on important time steps\n",
    "   - Dropout for regularization\n",
    "   - Separate regression heads for temperature and moisture prediction\n",
    "\n",
    "2. Training Infrastructure:\n",
    "   - Comprehensive training loop with validation\n",
    "   - Early stopping to prevent overfitting\n",
    "   - Learning rate optimization with Adam\n",
    "   - Loss tracking and visualization\n",
    "\n",
    "To use this model:\n",
    "\n",
    "1. First, prepare your data:\n",
    "```python\n",
    "dataset_path = \"path_to_your_dataset.csv\"\n",
    "dataset, full_dataloader = create_compost_dataloader(dataset_path)\n",
    "\n",
    "# Split into train/validation sets\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "```\n",
    "\n",
    "2. Train the model:\n",
    "```python\n",
    "model = train_compost_model(train_loader, val_loader)\n",
    "```\n",
    "\n",
    "3. Make predictions:\n",
    "```python\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    sample_input = next(iter(val_loader))[0]\n",
    "    predictions = model(sample_input)\n",
    "    # predictions[:, 0] is temperature\n",
    "    # predictions[:, 1] is moisture content\n",
    "```\n",
    "\n",
    "Next steps could include:\n",
    "1. Hyperparameter tuning\n",
    "2. Adding more prediction targets\n",
    "3. Implementing real-time monitoring\n",
    "4. Creating a user interface for the model\n",
    "5. Adding interpretability features\n",
    "\n",
    "Would you like me to implement any of these next steps or explain any part of the current implementation in more detail?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".flask-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
