{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Define dataset path dynamically\n",
    "root_dir = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "dataset_path: str = os.path.join(root_dir, \"data\", \"smart_compost_dataset101.csv\")\n",
    "\n",
    "\n",
    "class CompostDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        # Load data using pandas\n",
    "        df = pd.read_csv(dataset_path, delimiter=\",\")\n",
    "\n",
    "        # Select numerical columns for features (X) and target (y)\n",
    "        # Assuming the target variable is the first column (e.g., \"Temperature\")\n",
    "        self.y = torch.tensor(df.iloc[:, 0].values, dtype=torch.float32)  # Target\n",
    "        self.x = torch.tensor(df.iloc[:, 1:].values, dtype=torch.float32)  # Features\n",
    "        self.n_samples = df.shape[0]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Return a single sample\n",
    "        return self.x[index], self.y[index]\n",
    "\n",
    "\n",
    "# Instantiate dataset\n",
    "dataset = CompostDataset()\n",
    "first_data = dataset[0]\n",
    "features, labels = first_data\n",
    "print(\"Features:\", features)\n",
    "print(\"Labels:\", labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class CompostDataset(Dataset):\n",
    "    def __init__(self, csv_path):\n",
    "        # Load data\n",
    "        df = pd.read_csv(csv_path, delimiter=\",\", skip_blank_lines=True)\n",
    "\n",
    "        # Clean column names\n",
    "        df.columns = df.columns.str.strip()\n",
    "\n",
    "        # Remove any metadata or header rows (keeping only the actual data)\n",
    "        df = df[df[\"Variable\"].notna()]\n",
    "\n",
    "        # Create a mapping of variable names to numeric indices\n",
    "        self.variable_mapping = {\n",
    "            var: idx for idx, var in enumerate(df[\"Variable\"].unique())\n",
    "        }\n",
    "\n",
    "        # Convert optimal ranges to numeric values (taking the midpoint)\n",
    "        def extract_range_midpoint(range_str):\n",
    "            try:\n",
    "                # Handle special cases like '>7'\n",
    "                if \">\" in range_str:\n",
    "                    return float(range_str.replace(\">\", \"\"))\n",
    "\n",
    "                # Handle normal ranges like '45-65'\n",
    "                low, high = map(float, range_str.split(\"-\"))\n",
    "                return (low + high) / 2\n",
    "            except:\n",
    "                return None\n",
    "\n",
    "        df[\"OptimalRange\"] = (\n",
    "            df[\"OptimalRange\"].str.strip().apply(extract_range_midpoint)\n",
    "        )\n",
    "\n",
    "        # Select numeric columns for the feature set\n",
    "        numeric_columns = [\"OptimalRange\"]  # Add other numeric columns as needed\n",
    "\n",
    "        # Convert to numeric, handling errors\n",
    "        for col in numeric_columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "\n",
    "        # Drop rows with NaN values\n",
    "        df = df.dropna(subset=numeric_columns)\n",
    "\n",
    "        if df.empty:\n",
    "            raise ValueError(\n",
    "                \"Dataset is empty after preprocessing. Please check your data format.\"\n",
    "            )\n",
    "\n",
    "        # Store processed data\n",
    "        self.variable_names = df[\"Variable\"].values\n",
    "        self.features = torch.tensor(df[numeric_columns].values, dtype=torch.float32)\n",
    "        self.n_samples = len(df)\n",
    "\n",
    "        # Store additional metadata that might be useful\n",
    "        self.metadata = df[\n",
    "            [\"Type\", \"Unit\", \"Dependencies\", \"IntroductionStage\", \"Frequency\", \"Notes\"]\n",
    "        ]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        variable_name = self.variable_names[index]\n",
    "        variable_idx = torch.tensor(\n",
    "            self.variable_mapping[variable_name], dtype=torch.long\n",
    "        )\n",
    "        return self.features[index], variable_idx\n",
    "\n",
    "    def get_metadata(self, index):\n",
    "        \"\"\"Return metadata for a given index\"\"\"\n",
    "        return self.metadata.iloc[index]\n",
    "\n",
    "\n",
    "\n",
    "root_dir = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "dataset_path = os.path.join(root_dir, \"data\", \"smart_compost_dataset101.csv\")\n",
    "\n",
    "try:\n",
    "    dataset = CompostDataset(dataset_path)\n",
    "    # print(f\"Dataset size: {len(dataset)}\")\n",
    "\n",
    "    # Get first item\n",
    "    # features, variable_idx = dataset[0]\n",
    "    # print(f\"Features for first item: {features}\")\n",
    "    # print(f\"Variable index for first item: {variable_idx}\")\n",
    "\n",
    "    # Get metadata for first item\n",
    "    metadata = dataset.get_metadata(0)\n",
    "    # print(f\"Metadata for first item: {metadata}\")\n",
    "\n",
    "\n",
    "    # dataloader\n",
    "    dataloader =  DataLoader(dataset=dataset, batch_size=4, shuffle=True, num_workers=2)\n",
    "\n",
    "\n",
    "    datatiter  = iter(dataloader)\n",
    "    data  =  datatiter.__next__()\n",
    "    features, labels = data\n",
    "\n",
    "    print(f'features: {features}')\n",
    "    print(f'Labels : {labels}')\n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class CompostDataset(Dataset):\n",
    "    def __init__(self, csv_path):\n",
    "        # Load data\n",
    "        df = pd.read_csv(csv_path, delimiter=\",\", skip_blank_lines=True)\n",
    "\n",
    "        # Clean column names\n",
    "        df.columns = df.columns.str.strip()\n",
    "\n",
    "        # Remove any metadata or header rows (keeping only the actual data)\n",
    "        df = df[df[\"Variable\"].notna()]\n",
    "\n",
    "        # Create a mapping of variable names to numeric indices\n",
    "        self.variable_mapping = {\n",
    "            var: idx for idx, var in enumerate(df[\"Variable\"].unique())\n",
    "        }\n",
    "\n",
    "        # Convert optimal ranges to numeric values (taking the midpoint)\n",
    "        def extract_range_midpoint(range_str):\n",
    "            try:\n",
    "                if \">\" in range_str:\n",
    "                    return float(range_str.replace(\">\", \"\"))\n",
    "                low, high = map(float, range_str.split(\"-\"))\n",
    "                return (low + high) / 2\n",
    "            except:\n",
    "                return None\n",
    "\n",
    "        df[\"OptimalRange\"] = (\n",
    "            df[\"OptimalRange\"].str.strip().apply(extract_range_midpoint)\n",
    "        )\n",
    "\n",
    "        # Encode categorical columns as numeric\n",
    "        df[\"Frequency\"] = df[\"Frequency\"].astype(\"category\").cat.codes\n",
    "        df[\"IntroductionStage\"] = df[\"IntroductionStage\"].astype(\"category\").cat.codes\n",
    "\n",
    "        # Add these columns to the feature set\n",
    "        numeric_columns = [\"OptimalRange\", \"Frequency\", \"IntroductionStage\"]\n",
    "\n",
    "        # Normalize features\n",
    "        df[numeric_columns] = df[numeric_columns].apply(\n",
    "            lambda x: (x - x.mean()) / x.std()\n",
    "        )\n",
    "\n",
    "        # Convert to numeric and handle errors\n",
    "        for col in numeric_columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "\n",
    "        # Drop rows with NaN values\n",
    "        df = df.dropna(subset=numeric_columns)\n",
    "\n",
    "        if df.empty:\n",
    "            raise ValueError(\n",
    "                \"Dataset is empty after preprocessing. Please check your data format.\"\n",
    "            )\n",
    "\n",
    "        # Store processed data\n",
    "        self.variable_names = df[\"Variable\"].values\n",
    "        self.features = torch.tensor(df[numeric_columns].values, dtype=torch.float32)\n",
    "        self.labels = torch.tensor(\n",
    "            [self.variable_mapping[var] for var in df[\"Variable\"].values],\n",
    "            dtype=torch.long,\n",
    "        )\n",
    "        self.n_samples = len(df)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.features[index], self.labels[index]\n",
    "\n",
    "\n",
    "root_dir = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "dataset_path = os.path.join(root_dir, \"data\", \"smart_compost_dataset101.csv\")\n",
    "\n",
    "try:\n",
    "    dataset = CompostDataset(dataset_path)\n",
    "\n",
    "    # DataLoader\n",
    "    dataloader = DataLoader(dataset=dataset, batch_size=4, shuffle=True, num_workers=2)\n",
    "\n",
    "    data_iter = iter(dataloader)\n",
    "    features, labels = next(data_iter)\n",
    "\n",
    "    print(f\"Features: {features}\")\n",
    "    print(f\"Labels: {labels}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class CompostDataset(Dataset):\n",
    "    def __init__(self, csv_path):\n",
    "        # Load data\n",
    "        df = pd.read_csv(csv_path, delimiter=\",\", skip_blank_lines=True)\n",
    "\n",
    "        # Clean column names\n",
    "        df.columns = df.columns.str.strip()\n",
    "\n",
    "        # Remove any metadata or header rows\n",
    "        df = df[df[\"Variable\"].notna()]\n",
    "\n",
    "        # Create feature mappings\n",
    "        self.variable_mapping = {\n",
    "            var: idx for idx, var in enumerate(df[\"Variable\"].unique())\n",
    "        }\n",
    "\n",
    "        # Convert all ranges to numeric values\n",
    "        def extract_range_values(range_str):\n",
    "            try:\n",
    "                if isinstance(range_str, str):\n",
    "                    range_str = range_str.strip()\n",
    "                    if \">\" in range_str:\n",
    "                        return (\n",
    "                            float(range_str.replace(\">\", \"\")),\n",
    "                            float(range_str.replace(\">\", \"\")) * 1.2,\n",
    "                        )\n",
    "                    elif \":\" in range_str:  # Handle ratios like \"25-30:1\"\n",
    "                        range_part = range_str.split(\":\")[0]\n",
    "                        low, high = map(float, range_part.split(\"-\"))\n",
    "                        return low, high\n",
    "                    else:\n",
    "                        low, high = map(float, range_str.split(\"-\"))\n",
    "                        return low, high\n",
    "                return None, None\n",
    "            except:\n",
    "                return None, None\n",
    "\n",
    "        # Extract ranges and create feature bounds\n",
    "        ranges = df[\"OptimalRange\"].apply(extract_range_values)\n",
    "        df[\"lower_bound\"] = ranges.apply(lambda x: x[0])\n",
    "        df[\"upper_bound\"] = ranges.apply(lambda x: x[1])\n",
    "\n",
    "        # Create feature matrix\n",
    "        feature_cols = [\"lower_bound\", \"upper_bound\"]\n",
    "        self.features = torch.tensor(df[feature_cols].values, dtype=torch.float32)\n",
    "\n",
    "        # Create normalized features\n",
    "        self.features = self.normalize_features(self.features)\n",
    "\n",
    "        # Store variable information\n",
    "        self.variables = df[\"Variable\"].values\n",
    "        self.units = df[\"Unit\"].values\n",
    "        self.types = df[\"Type\"].values\n",
    "        self.dependencies = df[\"Dependencies\"].str.split(\"|\").values\n",
    "        self.n_samples = len(df)\n",
    "\n",
    "        # Store metadata\n",
    "        self.metadata = {\n",
    "            \"variable_names\": self.variables,\n",
    "            \"units\": self.units,\n",
    "            \"types\": self.types,\n",
    "            \"dependencies\": self.dependencies,\n",
    "        }\n",
    "\n",
    "    def normalize_features(self, features):\n",
    "        \"\"\"Normalize features to [0,1] range\"\"\"\n",
    "        min_vals = features.min(dim=0)[0]\n",
    "        max_vals = features.max(dim=0)[0]\n",
    "        return (features - min_vals) / (max_vals - min_vals)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Return features and variable index for the given index\"\"\"\n",
    "        features = self.features[index]\n",
    "        variable_idx = torch.tensor(\n",
    "            self.variable_mapping[self.variables[index]], dtype=torch.long\n",
    "        )\n",
    "\n",
    "        # Create dependency mask\n",
    "        deps = self.dependencies[index]\n",
    "        if isinstance(deps, (list, np.ndarray)):\n",
    "            dep_mask = torch.zeros(len(self.variable_mapping))\n",
    "            for dep in deps:\n",
    "                if dep in self.variable_mapping and dep != \"All_Above_Variables\":\n",
    "                    dep_mask[self.variable_mapping[dep]] = 1\n",
    "            return features, variable_idx, dep_mask\n",
    "        return features, variable_idx, torch.zeros(len(self.variable_mapping))\n",
    "\n",
    "    def get_variable_name(self, index):\n",
    "        \"\"\"Get variable name for given index\"\"\"\n",
    "        return self.variables[index]\n",
    "\n",
    "    def get_unit(self, index):\n",
    "        \"\"\"Get unit for given index\"\"\"\n",
    "        return self.units[index]\n",
    "\n",
    "    def get_dependencies(self, index):\n",
    "        \"\"\"Get dependencies for given index\"\"\"\n",
    "        return self.dependencies[index]\n",
    "\n",
    "    def get_optimal_range(self, index):\n",
    "        \"\"\"Get optimal range for given index\"\"\"\n",
    "        return self.features[index].numpy()\n",
    "\n",
    "\n",
    "# Example usage\n",
    "def create_dataloader(dataset_path, batch_size=4, shuffle=True):\n",
    "    try:\n",
    "        dataset = CompostDataset(dataset_path)\n",
    "        dataloader = DataLoader(\n",
    "            dataset=dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=shuffle,\n",
    "            num_workers=2,\n",
    "            collate_fn=lambda x: tuple(zip(*x)),  # Handle variable length dependencies\n",
    "        )\n",
    "        return dataset, dataloader\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating dataloader: {e}\")\n",
    "        return None, None\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    root_dir = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "    dataset_path = os.path.join(root_dir, \"data\", \"smart_compost_dataset101.csv\")\n",
    "\n",
    "    dataset, dataloader = create_dataloader(dataset_path)\n",
    "    if dataloader:\n",
    "        # Get a batch of data\n",
    "        data_iter = iter(dataloader)\n",
    "        features, variable_indices, dependency_masks = next(data_iter)\n",
    "\n",
    "        print(\"Batch features shape:\", torch.stack(features).shape)\n",
    "        print(\"Batch variable indices:\", variable_indices)\n",
    "        print(\"First item dependencies:\", dataset.get_dependencies(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset and dataloader\n",
    "dataset, dataloader = create_compost_dataloader(\n",
    "    dataset_path, batch_size=32, sequence_length=30\n",
    ")\n",
    "\n",
    "# Training loop example\n",
    "for batch_features, batch_targets in dataloader:\n",
    "    # batch_features: shape [batch_size, sequence_length, n_features]\n",
    "    # batch_targets: shape [batch_size, n_targets]\n",
    "\n",
    "    # Features include:\n",
    "    # - Measured values for all variables\n",
    "    # - Waste type encoding\n",
    "    # - Weather data\n",
    "\n",
    "    # Targets are next-day predictions for:\n",
    "    # - Temperature\n",
    "    # - Moisture content\n",
    "\n",
    "    # Your model training code here\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "looping ...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 265\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlooping ...\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# Get a batch of data\u001b[39;00m\n\u001b[0;32m--> 265\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_targets\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    266\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mBatch features shape:\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_features\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mBatch targets shape:\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_targets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Softwares/University/smart-compost/flask-venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    707\u001b[0m ):\n",
      "File \u001b[0;32m~/Softwares/University/smart-compost/flask-venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1448\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1445\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[1;32m   1447\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1448\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1449\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1450\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[1;32m   1451\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m~/Softwares/University/smart-compost/flask-venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1412\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1408\u001b[0m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[1;32m   1409\u001b[0m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[1;32m   1410\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1411\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m-> 1412\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1413\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m   1414\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/Softwares/University/smart-compost/flask-venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1243\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1230\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m_utils\u001b[38;5;241m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[1;32m   1231\u001b[0m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[1;32m   1232\u001b[0m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1240\u001b[0m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[1;32m   1241\u001b[0m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[1;32m   1242\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1243\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1244\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[1;32m   1245\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1246\u001b[0m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[1;32m   1247\u001b[0m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[1;32m   1248\u001b[0m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.12/multiprocessing/queues.py:113\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m block:\n\u001b[1;32m    112\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m deadline \u001b[38;5;241m-\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic()\n\u001b[0;32m--> 113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    114\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll():\n",
      "File \u001b[0;32m/usr/lib/python3.12/multiprocessing/connection.py:257\u001b[0m, in \u001b[0;36m_ConnectionBase.poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_closed()\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_readable()\n\u001b[0;32m--> 257\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.12/multiprocessing/connection.py:440\u001b[0m, in \u001b[0;36mConnection._poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_poll\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout):\n\u001b[0;32m--> 440\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    441\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(r)\n",
      "File \u001b[0;32m/usr/lib/python3.12/multiprocessing/connection.py:1136\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     deadline \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic() \u001b[38;5;241m+\u001b[39m timeout\n\u001b[1;32m   1135\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m-> 1136\u001b[0m     ready \u001b[38;5;241m=\u001b[39m \u001b[43mselector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1137\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ready:\n\u001b[1;32m   1138\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [key\u001b[38;5;241m.\u001b[39mfileobj \u001b[38;5;28;01mfor\u001b[39;00m (key, events) \u001b[38;5;129;01min\u001b[39;00m ready]\n",
      "File \u001b[0;32m/usr/lib/python3.12/selectors.py:415\u001b[0m, in \u001b[0;36m_PollLikeSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    413\u001b[0m ready \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    414\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 415\u001b[0m     fd_event_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_selector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[1;32m    417\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "\n",
    "class WasteType:\n",
    "    \"\"\"Classification of different waste types and their properties\"\"\"\n",
    "\n",
    "    TYPES = {\n",
    "        \"GREEN\": {\n",
    "            \"examples\": [\"grass\", \"vegetable_scraps\", \"coffee_grounds\", \"fruit_waste\"],\n",
    "            \"nitrogen_content\": \"high\",\n",
    "            \"moisture_content\": \"high\",\n",
    "            \"decomposition_rate\": \"fast\",\n",
    "        },\n",
    "        \"BROWN\": {\n",
    "            \"examples\": [\"leaves\", \"straw\", \"paper\", \"cardboard\", \"wood_chips\"],\n",
    "            \"nitrogen_content\": \"low\",\n",
    "            \"moisture_content\": \"low\",\n",
    "            \"decomposition_rate\": \"slow\",\n",
    "        },\n",
    "        \"MIXED\": {\n",
    "            \"examples\": [\"food_scraps\", \"garden_waste\"],\n",
    "            \"nitrogen_content\": \"medium\",\n",
    "            \"moisture_content\": \"medium\",\n",
    "            \"decomposition_rate\": \"medium\",\n",
    "        },\n",
    "    }\n",
    "\n",
    "    @staticmethod\n",
    "    def get_type_encoding(waste_type: str) -> torch.Tensor:\n",
    "        \"\"\"Convert waste type to one-hot encoding\"\"\"\n",
    "        encoding = torch.zeros(len(WasteType.TYPES))\n",
    "        if waste_type in WasteType.TYPES:\n",
    "            encoding[list(WasteType.TYPES.keys()).index(waste_type)] = 1\n",
    "        return encoding\n",
    "\n",
    "\n",
    "class SeasonalEffects:\n",
    "    \"\"\"Handle seasonal and weather effects on composting\"\"\"\n",
    "\n",
    "    SEASONS = {\n",
    "        \"SPRING\": {\"temp_modifier\": 0.2, \"moisture_modifier\": 0.3},\n",
    "        \"SUMMER\": {\"temp_modifier\": 0.4, \"moisture_modifier\": -0.2},\n",
    "        \"FALL\": {\"temp_modifier\": 0.0, \"moisture_modifier\": 0.1},\n",
    "        \"WINTER\": {\"temp_modifier\": -0.3, \"moisture_modifier\": 0.2},\n",
    "    }\n",
    "\n",
    "    @staticmethod\n",
    "    def get_season(date: datetime) -> str:\n",
    "        \"\"\"Determine season from date\"\"\"\n",
    "        month = date.month\n",
    "        if month in [3, 4, 5]:\n",
    "            return \"SPRING\"\n",
    "        elif month in [6, 7, 8]:\n",
    "            return \"SUMMER\"\n",
    "        elif month in [9, 10, 11]:\n",
    "            return \"FALL\"\n",
    "        else:\n",
    "            return \"WINTER\"\n",
    "\n",
    "\n",
    "class CompostTimeSeriesDataset(Dataset):\n",
    "    def __init__(self, csv_path: str, sequence_length: int = 30):\n",
    "        super().__init__()\n",
    "        self.sequence_length = sequence_length\n",
    "        self.base_data = pd.read_csv(csv_path, delimiter=\",\", skip_blank_lines=True)\n",
    "        self.base_data.columns = self.base_data.columns.str.strip()\n",
    "\n",
    "        # Initialize time series data structures\n",
    "        self.time_series_data = self._generate_time_series_data()\n",
    "        self.waste_types = self._initialize_waste_types()\n",
    "        self.weather_data = self._initialize_weather_data()\n",
    "\n",
    "        # Process optimal ranges\n",
    "        self._process_optimal_ranges()\n",
    "\n",
    "        # Create feature matrices\n",
    "        self._create_feature_matrices()\n",
    "\n",
    "    def _generate_time_series_data(self) -> Dict[str, List[float]]:\n",
    "        \"\"\"Generate synthetic time series data for each variable\"\"\"\n",
    "        time_series = {}\n",
    "        start_date = datetime.now() - timedelta(days=365)\n",
    "        dates = [start_date + timedelta(days=i) for i in range(365)]\n",
    "\n",
    "        for _, row in self.base_data.iterrows():\n",
    "            variable = row[\"Variable\"]\n",
    "            optimal_range = self._parse_range(row[\"OptimalRange\"])\n",
    "            if optimal_range:\n",
    "                mid_point = (optimal_range[0] + optimal_range[1]) / 2\n",
    "                # Generate daily values with realistic variations\n",
    "                values = [\n",
    "                    self._generate_realistic_value(mid_point, optimal_range, dates[i])\n",
    "                    for i in range(365)\n",
    "                ]\n",
    "                time_series[variable] = values\n",
    "\n",
    "        return time_series\n",
    "\n",
    "    def _generate_realistic_value(\n",
    "        self, mid_point: float, optimal_range: Tuple[float, float], date: datetime\n",
    "    ) -> float:\n",
    "        \"\"\"Generate realistic value considering seasonal effects and daily variations\"\"\"\n",
    "        season = SeasonalEffects.get_season(date)\n",
    "        season_effect = SeasonalEffects.SEASONS[season][\"temp_modifier\"]\n",
    "\n",
    "        # Add daily variation (random walk with bounds)\n",
    "        daily_variation = np.random.normal(0, 0.05)\n",
    "        value = mid_point * (1 + season_effect + daily_variation)\n",
    "\n",
    "        # Ensure value stays within optimal range\n",
    "        return max(optimal_range[0], min(optimal_range[1], value))\n",
    "\n",
    "    def _initialize_waste_types(self) -> List[str]:\n",
    "        \"\"\"Initialize waste type sequence\"\"\"\n",
    "        return np.random.choice(list(WasteType.TYPES.keys()), size=365).tolist()\n",
    "\n",
    "    def _initialize_weather_data(self) -> Dict[str, List[float]]:\n",
    "        \"\"\"Generate synthetic weather data\"\"\"\n",
    "        weather_data = {\"temperature\": [], \"humidity\": [], \"rainfall\": []}\n",
    "\n",
    "        start_date = datetime.now() - timedelta(days=365)\n",
    "        for i in range(365):\n",
    "            date = start_date + timedelta(days=i)\n",
    "            season = SeasonalEffects.get_season(date)\n",
    "            season_effects = SeasonalEffects.SEASONS[season]\n",
    "\n",
    "            # Generate weather data with seasonal effects\n",
    "            weather_data[\"temperature\"].append(\n",
    "                20 + season_effects[\"temp_modifier\"] * 30 + np.random.normal(0, 2)\n",
    "            )\n",
    "            weather_data[\"humidity\"].append(\n",
    "                60 + season_effects[\"moisture_modifier\"] * 30 + np.random.normal(0, 5)\n",
    "            )\n",
    "            weather_data[\"rainfall\"].append(\n",
    "                max(\n",
    "                    0,\n",
    "                    np.random.exponential(scale=2)\n",
    "                    * (1 + season_effects[\"moisture_modifier\"]),\n",
    "                )\n",
    "            )\n",
    "\n",
    "        return weather_data\n",
    "\n",
    "    def _parse_range(self, range_str: str) -> Tuple[float, float]:\n",
    "        \"\"\"Parse range string to tuple of floats\"\"\"\n",
    "        try:\n",
    "            if isinstance(range_str, str):\n",
    "                range_str = range_str.strip()\n",
    "                if \">\" in range_str:\n",
    "                    val = float(range_str.replace(\">\", \"\"))\n",
    "                    return (val, val * 1.2)\n",
    "                elif \":\" in range_str:\n",
    "                    range_part = range_str.split(\":\")[0]\n",
    "                    low, high = map(float, range_part.split(\"-\"))\n",
    "                    return (low, high)\n",
    "                else:\n",
    "                    return tuple(map(float, range_str.split(\"-\")))\n",
    "            return None\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "    def _process_optimal_ranges(self):\n",
    "        \"\"\"Process optimal ranges for all variables\"\"\"\n",
    "        self.optimal_ranges = {}\n",
    "        for _, row in self.base_data.iterrows():\n",
    "            range_vals = self._parse_range(row[\"OptimalRange\"])\n",
    "            if range_vals:\n",
    "                self.optimal_ranges[row[\"Variable\"]] = range_vals\n",
    "\n",
    "    def _create_feature_matrices(self):\n",
    "        \"\"\"Create feature matrices for the dataset\"\"\"\n",
    "        self.features = []\n",
    "        self.targets = []\n",
    "\n",
    "        for i in range(\n",
    "            len(self.time_series_data[\"Temperature\"]) - self.sequence_length\n",
    "        ):\n",
    "            # Create sequence of features\n",
    "            sequence = []\n",
    "            for day in range(self.sequence_length):\n",
    "                idx = i + day\n",
    "\n",
    "                # Measurements\n",
    "                measurements = [\n",
    "                    self.time_series_data[var][idx]\n",
    "                    for var in self.time_series_data.keys()\n",
    "                ]\n",
    "\n",
    "                # Waste type encoding\n",
    "                waste_encoding = WasteType.get_type_encoding(self.waste_types[idx])\n",
    "\n",
    "                # Weather data\n",
    "                weather = [self.weather_data[w][idx] for w in self.weather_data.keys()]\n",
    "\n",
    "                # Combine all features\n",
    "                day_features = measurements + waste_encoding.tolist() + weather\n",
    "                sequence.append(day_features)\n",
    "\n",
    "            self.features.append(torch.tensor(sequence, dtype=torch.float32))\n",
    "\n",
    "            # Target is the next day's temperature and moisture content\n",
    "            target_idx = i + self.sequence_length\n",
    "            target = [\n",
    "                self.time_series_data[\"Temperature\"][target_idx],\n",
    "                self.time_series_data[\"Moisture_Content\"][target_idx],\n",
    "            ]\n",
    "            self.targets.append(torch.tensor(target, dtype=torch.float32))\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        return self.features[idx], self.targets[idx]\n",
    "\n",
    "    def get_optimal_ranges(self) -> Dict[str, Tuple[float, float]]:\n",
    "        \"\"\"Return optimal ranges for all variables\"\"\"\n",
    "        return self.optimal_ranges\n",
    "\n",
    "    def get_variable_metadata(self) -> pd.DataFrame:\n",
    "        \"\"\"Return metadata for all variables\"\"\"\n",
    "        return self.base_data[\n",
    "            [\n",
    "                \"Variable\",\n",
    "                \"Type\",\n",
    "                \"Unit\",\n",
    "                \"Dependencies\",\n",
    "                \"IntroductionStage\",\n",
    "                \"Frequency\",\n",
    "                \"Notes\",\n",
    "            ]\n",
    "        ]\n",
    "\n",
    "\n",
    "# Example usage\n",
    "def create_compost_dataloader(\n",
    "    dataset_path: str, batch_size: int = 32, sequence_length: int = 30\n",
    "):\n",
    "    try:\n",
    "        dataset = CompostTimeSeriesDataset(\n",
    "            dataset_path, sequence_length=sequence_length\n",
    "        )\n",
    "        dataloader = DataLoader(\n",
    "            dataset=dataset, batch_size=batch_size, shuffle=True, num_workers=2\n",
    "        )\n",
    "        return dataset, dataloader\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating dataloader: {e}\")\n",
    "        return None, None\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    root_dir = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "    dataset_path = os.path.join(root_dir, \"data\", \"smart_compost_dataset101.csv\")\n",
    "\n",
    "    dataset, dataloader = create_compost_dataloader(dataset_path)\n",
    "    if dataloader:\n",
    "        print('looping ...')\n",
    "        # Get a batch of data\n",
    "        for batch_features, batch_targets in dataloader:\n",
    "            print(\"Batch features shape:\", batch_features.shape)\n",
    "            print(\"Batch targets shape:\", batch_targets.shape)\n",
    "            print(\n",
    "                \"\\nFeature sequence (first day of first sample):\", batch_features[0][0]\n",
    "            )\n",
    "            print(\"Target values:\", batch_targets[0])\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "from typing import Tuple, Dict\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class CompostLSTM(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size: int,\n",
    "        hidden_size: int = 64,\n",
    "        num_layers: int = 2,\n",
    "        dropout: float = 0.2,\n",
    "    ):\n",
    "        super(CompostLSTM, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # LSTM layer\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "        )\n",
    "\n",
    "        # Attention layer\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size), nn.Tanh(), nn.Linear(hidden_size, 1)\n",
    "        )\n",
    "\n",
    "        # Output layers for temperature and moisture prediction\n",
    "        self.regression_head = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_size // 2, 2),  # 2 outputs: temperature and moisture\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # LSTM forward pass\n",
    "        lstm_out, _ = self.lstm(x)  # Shape: [batch, seq_len, hidden_size]\n",
    "\n",
    "        # Apply attention\n",
    "        attention_weights = self.attention(lstm_out)  # Shape: [batch, seq_len, 1]\n",
    "        attention_weights = torch.softmax(attention_weights, dim=1)\n",
    "\n",
    "        # Weighted sum of LSTM outputs\n",
    "        context = torch.sum(\n",
    "            attention_weights * lstm_out, dim=1\n",
    "        )  # Shape: [batch, hidden_size]\n",
    "\n",
    "        # Generate predictions\n",
    "        predictions = self.regression_head(context)  # Shape: [batch, 2]\n",
    "\n",
    "        return predictions\n",
    "\n",
    "\n",
    "class CompostModelTrainer:\n",
    "    def __init__(self, model: nn.Module, learning_rate: float = 0.001):\n",
    "        self.model = model\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model.to(self.device)\n",
    "\n",
    "        # Training history\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "\n",
    "    def train_epoch(self, train_loader: DataLoader) -> float:\n",
    "        self.model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        for batch_features, batch_targets in train_loader:\n",
    "            batch_features = batch_features.to(self.device)\n",
    "            batch_targets = batch_targets.to(self.device)\n",
    "\n",
    "            # Forward pass\n",
    "            self.optimizer.zero_grad()\n",
    "            predictions = self.model(batch_features)\n",
    "            loss = self.criterion(predictions, batch_targets)\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        return total_loss / len(train_loader)\n",
    "\n",
    "    def validate(self, val_loader: DataLoader) -> float:\n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch_features, batch_targets in val_loader:\n",
    "                batch_features = batch_features.to(self.device)\n",
    "                batch_targets = batch_targets.to(self.device)\n",
    "\n",
    "                predictions = self.model(batch_features)\n",
    "                loss = self.criterion(predictions, batch_targets)\n",
    "                total_loss += loss.item()\n",
    "\n",
    "        return total_loss / len(val_loader)\n",
    "\n",
    "    def train(\n",
    "        self,\n",
    "        train_loader: DataLoader,\n",
    "        val_loader: DataLoader,\n",
    "        epochs: int = 100,\n",
    "        early_stopping_patience: int = 10,\n",
    "    ) -> Dict:\n",
    "\n",
    "        best_val_loss = float(\"inf\")\n",
    "        patience_counter = 0\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            # Train and validate\n",
    "            train_loss = self.train_epoch(train_loader)\n",
    "            val_loss = self.validate(val_loader)\n",
    "\n",
    "            # Store losses\n",
    "            self.train_losses.append(train_loss)\n",
    "            self.val_losses.append(val_loss)\n",
    "\n",
    "            # Print progress\n",
    "            print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "            print(f\"Train Loss: {train_loss:.4f}\")\n",
    "            print(f\"Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "            # Early stopping check\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                patience_counter = 0\n",
    "                # Save best model\n",
    "                torch.save(self.model.state_dict(), \"best_compost_model.pth\")\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "\n",
    "            if patience_counter >= early_stopping_patience:\n",
    "                print(\"Early stopping triggered!\")\n",
    "                break\n",
    "\n",
    "        return {\"train_losses\": self.train_losses, \"val_losses\": self.val_losses}\n",
    "\n",
    "    def plot_training_history(self):\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(self.train_losses, label=\"Training Loss\")\n",
    "        plt.plot(self.val_losses, label=\"Validation Loss\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.title(\"Training History\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's what this model implementation provides:\n",
    "\n",
    "Model Architecture:\n",
    "\n",
    "LSTM-based network for time series processing\n",
    "Attention mechanism to focus on important time steps\n",
    "Dropout for regularization\n",
    "Separate regression heads for temperature and moisture prediction\n",
    "\n",
    "\n",
    "Training Infrastructure:\n",
    "\n",
    "Comprehensive training loop with validation\n",
    "Early stopping to prevent overfitting\n",
    "Learning rate optimization with Adam\n",
    "Loss tracking and visualization\n",
    "\n",
    "\n",
    "\n",
    "To use this model:\n",
    "\n",
    "First, prepare your data:\n",
    "```python \n",
    "dataset_path = \"path_to_your_dataset.csv\"\n",
    "dataset, full_dataloader = create_compost_dataloader(dataset_path)\n",
    "\n",
    "# Split into train/validation sets\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step would be to create a neural network model to train on this composting data. Let me create an appropriate model architecture for this task.\n",
    "\n",
    "\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "from typing import Tuple, Dict\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class CompostLSTM(nn.Module):\n",
    "    def __init__(self, input_size: int, hidden_size: int = 64, num_layers: int = 2, dropout: float = 0.2):\n",
    "        super(CompostLSTM, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # LSTM layer\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0\n",
    "        )\n",
    "\n",
    "        # Attention layer\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size, 1)\n",
    "        )\n",
    "\n",
    "        # Output layers for temperature and moisture prediction\n",
    "        self.regression_head = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_size // 2, 2)  # 2 outputs: temperature and moisture\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # LSTM forward pass\n",
    "        lstm_out, _ = self.lstm(x)  # Shape: [batch, seq_len, hidden_size]\n",
    "\n",
    "        # Apply attention\n",
    "        attention_weights = self.attention(lstm_out)  # Shape: [batch, seq_len, 1]\n",
    "        attention_weights = torch.softmax(attention_weights, dim=1)\n",
    "\n",
    "        # Weighted sum of LSTM outputs\n",
    "        context = torch.sum(attention_weights * lstm_out, dim=1)  # Shape: [batch, hidden_size]\n",
    "\n",
    "        # Generate predictions\n",
    "        predictions = self.regression_head(context)  # Shape: [batch, 2]\n",
    "\n",
    "        return predictions\n",
    "\n",
    "class CompostModelTrainer:\n",
    "    def __init__(self, model: nn.Module, learning_rate: float = 0.001):\n",
    "        self.model = model\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model.to(self.device)\n",
    "\n",
    "        # Training history\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "\n",
    "    def train_epoch(self, train_loader: DataLoader) -> float:\n",
    "        self.model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        for batch_features, batch_targets in train_loader:\n",
    "            batch_features = batch_features.to(self.device)\n",
    "            batch_targets = batch_targets.to(self.device)\n",
    "\n",
    "            # Forward pass\n",
    "            self.optimizer.zero_grad()\n",
    "            predictions = self.model(batch_features)\n",
    "            loss = self.criterion(predictions, batch_targets)\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        return total_loss / len(train_loader)\n",
    "\n",
    "    def validate(self, val_loader: DataLoader) -> float:\n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch_features, batch_targets in val_loader:\n",
    "                batch_features = batch_features.to(self.device)\n",
    "                batch_targets = batch_targets.to(self.device)\n",
    "\n",
    "                predictions = self.model(batch_features)\n",
    "                loss = self.criterion(predictions, batch_targets)\n",
    "                total_loss += loss.item()\n",
    "\n",
    "        return total_loss / len(val_loader)\n",
    "\n",
    "    def train(self,\n",
    "              train_loader: DataLoader,\n",
    "              val_loader: DataLoader,\n",
    "              epochs: int = 100,\n",
    "              early_stopping_patience: int = 10) -> Dict:\n",
    "\n",
    "        best_val_loss = float('inf')\n",
    "        patience_counter = 0\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            # Train and validate\n",
    "            train_loss = self.train_epoch(train_loader)\n",
    "            val_loss = self.validate(val_loader)\n",
    "\n",
    "            # Store losses\n",
    "            self.train_losses.append(train_loss)\n",
    "            self.val_losses.append(val_loss)\n",
    "\n",
    "            # Print progress\n",
    "            print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "            print(f\"Train Loss: {train_loss:.4f}\")\n",
    "            print(f\"Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "            # Early stopping check\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                patience_counter = 0\n",
    "                # Save best model\n",
    "                torch.save(self.model.state_dict(), 'best_compost_model.pth')\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "\n",
    "            if patience_counter >= early_stopping_patience:\n",
    "                print(\"Early stopping triggered!\")\n",
    "                break\n",
    "\n",
    "        return {\n",
    "            'train_losses': self.train_losses,\n",
    "            'val_losses': self.val_losses\n",
    "        }\n",
    "\n",
    "    def plot_training_history(self):\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(self.train_losses, label='Training Loss')\n",
    "        plt.plot(self.val_losses, label='Validation Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Training History')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "# Example usage\n",
    "def train_compost_model(train_loader: DataLoader, val_loader: DataLoader) -> CompostLSTM:\n",
    "    # Calculate input size from data\n",
    "    sample_features = next(iter(train_loader))[0]\n",
    "    input_size = sample_features.shape[-1]  # number of features\n",
    "\n",
    "    # Initialize model and trainer\n",
    "    model = CompostLSTM(input_size=input_size)\n",
    "    trainer = CompostModelTrainer(model)\n",
    "\n",
    "    # Train the model\n",
    "    history = trainer.train(\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        epochs=100,\n",
    "        early_stopping_patience=10\n",
    "    )\n",
    "\n",
    "    # Plot training history\n",
    "    trainer.plot_training_history()\n",
    "\n",
    "    return model\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Assuming you have already created your dataset and dataloaders\n",
    "    from compost_dataset import create_compost_dataloader\n",
    "\n",
    "    # Create dataset and split into train/val\n",
    "    dataset_path = \"path_to_your_dataset.csv\"\n",
    "    dataset, full_dataloader = create_compost_dataloader(dataset_path)\n",
    "\n",
    "    # Split data into train and validation sets\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    val_size = len(dataset) - train_size\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    # Train the model\n",
    "    model = train_compost_model(train_loader, val_loader)\n",
    "\n",
    "```\n",
    "\n",
    "Here's what this model implementation provides:\n",
    "\n",
    "1. Model Architecture:\n",
    "   - LSTM-based network for time series processing\n",
    "   - Attention mechanism to focus on important time steps\n",
    "   - Dropout for regularization\n",
    "   - Separate regression heads for temperature and moisture prediction\n",
    "\n",
    "2. Training Infrastructure:\n",
    "   - Comprehensive training loop with validation\n",
    "   - Early stopping to prevent overfitting\n",
    "   - Learning rate optimization with Adam\n",
    "   - Loss tracking and visualization\n",
    "\n",
    "To use this model:\n",
    "\n",
    "1. First, prepare your data:\n",
    "```python\n",
    "dataset_path = \"path_to_your_dataset.csv\"\n",
    "dataset, full_dataloader = create_compost_dataloader(dataset_path)\n",
    "\n",
    "# Split into train/validation sets\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "```\n",
    "\n",
    "2. Train the model:\n",
    "```python\n",
    "model = train_compost_model(train_loader, val_loader)\n",
    "```\n",
    "\n",
    "3. Make predictions:\n",
    "```python\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    sample_input = next(iter(val_loader))[0]\n",
    "    predictions = model(sample_input)\n",
    "    # predictions[:, 0] is temperature\n",
    "    # predictions[:, 1] is moisture content\n",
    "```\n",
    "\n",
    "Next steps could include:\n",
    "1. Hyperparameter tuning\n",
    "2. Adding more prediction targets\n",
    "3. Implementing real-time monitoring\n",
    "4. Creating a user interface for the model\n",
    "5. Adding interpretability features\n",
    "\n",
    "Would you like me to implement any of these next steps or explain any part of the current implementation in more detail?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flask-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
