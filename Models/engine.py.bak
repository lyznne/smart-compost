"""
SMART COMPOST - MODEL PROJECT.

---  the training of the model
---   Models/engine.py

* Author  -  enos muthiani
* git     -  https://github.com/lyznne
* date    - 3 Dec 2024
* email   - emuthiani26@gmail.com


                                    Copyright (c) 2024      - enos.vercel.app
"""

def train_compost_model(
    train_loader: DataLoader, val_loader: DataLoader
) -> CompostLSTM:
    # Calculate input size from data
    sample_features = next(iter(train_loader))[0]
    input_size = sample_features.shape[-1]  # number of features

    # Initialize model and trainer
    model = CompostLSTM(input_size=input_size)
    trainer = CompostModelTrainer(model)

    # Train the model
    history = trainer.train(
        train_loader=train_loader,
        val_loader=val_loader,
        epochs=100,
        early_stopping_patience=10,
    )

    # Plot training history
    trainer.plot_training_history()

    return model


if __name__ == "__main__":
    # Assuming you have already created your dataset and dataloaders
    from compost_dataset import create_compost_dataloader

    # Create dataset and split into train/val
    dataset_path = "path_to_your_dataset.csv"
    dataset, full_dataloader = create_compost_dataloader(dataset_path)

    # Split data into train and validation sets
    train_size = int(0.8 * len(dataset))
    val_size = len(dataset) - train_size
    train_dataset, val_dataset = torch.utils.data.random_split(
        dataset, [train_size, val_size]
    )

    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)

    # Train the model
    model = train_compost_model(train_loader, val_loader)

# Example usage
def create_compost_dataloader(
    dataset_path: str, batch_size: int = 32, sequence_length: int = 30
):
    try:
        dataset = CompostTimeSeriesDataset(
            dataset_path, sequence_length=sequence_length
        )
        dataloader = DataLoader(
            dataset=dataset, batch_size=batch_size, shuffle=True, num_workers=2
        )
        return dataset, dataloader
    except Exception as e:
        print(f"Error creating dataloader: {e}")
        return None, None


if __name__ == "__main__":
    root_dir = os.path.abspath(os.path.join(os.getcwd(), ".."))
    dataset_path = os.path.join(root_dir, "data", "smart_compost_dataset101.csv")

    dataset, dataloader = create_compost_dataloader(dataset_path)
    if dataloader:
        print("looping ...")
        # Get a batch of data
        for batch_features, batch_targets in dataloader:
            print("Batch features shape:", batch_features.shape)
            print("Batch targets shape:", batch_targets.shape)
            print(
                "\nFeature sequence (first day of first sample):", batch_features[0][0]
            )
            print("Target values:", batch_targets[0])
            break
